---
title: LangGraph-3-UX and Human-in-the-Loop
category: aigc
tags:
  - agent
  - ai
  - langgraph
  - python
---

æœ¬æ¨¡å—èšç„¦äºäººæœºåä½œï¼Œå®ƒåŸºäºè®°å¿†åŠŸèƒ½ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç›´æ¥ä¸å›¾è¿›è¡Œå¤šæ ·çš„äº¤äº’ã€‚ä¸ºäº†æ›´å¥½åœ°å¼•å…¥äººæœºåä½œï¼Œå°†å…ˆä»‹ç»æµå¼å¤„ç†ï¼Œå®ƒå¯ä»¥åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä»¥å¤šç§æ–¹å¼å±•ç¤ºå›¾çš„è¾“å‡ºï¼Œå¦‚èŠ‚ç‚¹çŠ¶æ€æˆ–èŠå¤©æ¨¡å‹çš„æ ‡è®°ç­‰ã€‚

- **äººæœºåä½œï¼ˆhuman-in-the-loopï¼‰**ï¼šè¿™æ˜¯ä¸€ç§å°†äººç±»çš„æ™ºæ…§å’Œå†³ç­–èå…¥åˆ°è‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­çš„æ–¹æ³•ã€‚åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œå®ƒæ„å‘³ç€ç”¨æˆ·èƒ½å¤Ÿç§¯æå‚ä¸åˆ°å›¾çš„äº¤äº’è¿‡ç¨‹ä¸­ï¼Œè€Œä¸æ˜¯å®Œå…¨ä¾èµ–è‡ªåŠ¨åŒ–ç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®å›¾çš„å®æ—¶çŠ¶æ€åšå‡ºå†³ç­–ï¼Œæˆ–è€…å¯¹å›¾çš„æŸäº›éƒ¨åˆ†è¿›è¡Œæ‰‹åŠ¨è°ƒæ•´ã€‚
- **åŸºäºè®°å¿†åŠŸèƒ½**ï¼šè¿™è¡¨æ˜äººæœºåä½œçš„å®ç°ä¾èµ–äºä¹‹å‰æåˆ°çš„è®°å¿†åŠŸèƒ½ã€‚è®°å¿†åŠŸèƒ½å¯èƒ½æ˜¯æŒ‡ç³»ç»Ÿèƒ½å¤Ÿå­˜å‚¨å’Œå›å¿†ä¹‹å‰çš„ä¿¡æ¯ï¼Œè¿™ä½¿å¾—ç”¨æˆ·åœ¨ä¸å›¾äº¤äº’æ—¶èƒ½å¤ŸåŸºäºè¿™äº›è®°å¿†è¿›è¡Œæ“ä½œï¼Œä»è€Œå®ç°æ›´è¿è´¯ã€æ›´ä¸ªæ€§åŒ–çš„äº¤äº’ä½“éªŒã€‚
- **ç›´æ¥ä¸å›¾äº¤äº’**ï¼šç”¨æˆ·å¯ä»¥ç›´æ¥å¯¹å›¾è¿›è¡Œæ“ä½œï¼Œè€Œä¸æ˜¯é€šè¿‡å¤æ‚çš„ç•Œé¢æˆ–é—´æ¥çš„æ–¹å¼ã€‚è¿™ç§ç›´æ¥äº¤äº’å¯ä»¥æ˜¯ä¿®æ”¹å›¾çš„ç»“æ„ã€è°ƒæ•´èŠ‚ç‚¹çš„å±æ€§ï¼Œæˆ–è€…æ ¹æ®å›¾çš„è¾“å‡ºåšå‡ºå³æ—¶åé¦ˆã€‚
- **æµå¼å¤„ç†ï¼ˆstreamingï¼‰**ï¼šè¿™æ˜¯ä¸€ç§æ•°æ®å¤„ç†æ–¹å¼ï¼Œæ•°æ®ä»¥è¿ç»­çš„æµçš„å½¢å¼è¢«å¤„ç†ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ•°æ®ã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæµå¼å¤„ç†ç”¨äºå®æ—¶å±•ç¤ºå›¾çš„è¾“å‡ºï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿçœ‹åˆ°å›¾åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚

# Streaming

## æµå¼è¾“å‡º

`.stream` å’Œ .`astream` æ˜¯ç”¨äºå›ä¼ ç»“æœçš„åŒæ­¥å’Œå¼‚æ­¥æ–¹æ³•ã€‚

LangGraph æ”¯æŒå‡ ç§ä¸åŒçš„å›¾çŠ¶æ€æµæ¨¡å¼ï¼š

* `values`ï¼šæ­¤æ“ä½œåœ¨è°ƒç”¨æ¯ä¸ªèŠ‚ç‚¹ä¹‹åä¼šæµå¼ä¼ è¾“æ•´ä¸ªå›¾çš„çŠ¶æ€ã€‚
* `updates`ï¼šæ­¤æ“ä½œåœ¨è°ƒç”¨æ¯ä¸ªèŠ‚ç‚¹ä¹‹åä¼šæµå¼ä¼ è¾“å›¾çŠ¶æ€çš„æ›´æ–°ã€‚

```python
# Create a thread
config = {"configurable": {"thread_id": "1"}}

# Start conversation
for chunk in graph.stream({"messages": [HumanMessage(content="hi! I'm Lance")]}, config, stream_mode="updates"):
    print(chunk)
```

```
{'conversation': {'messages': AIMessage(content='Hi Lance! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-6d58e31e-a278-4df6-ab0a-bb51d08ca037-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21})}}
```

æ‰“å°stateçš„æ›´æ–°


```
================================== Ai Message ==================================
Hi Lance! How are you doing today?
```

å¯ä»¥çœ‹åˆ°ï¼Œç”¨æˆ·è¾“å…¥`"hi! I'm Lance"`,AIå›å¤`Hi Lance! How can I assist you today?'`ï¼Œåœ¨å½“å‰çš„updateæ¨¡å¼ä¸‹ï¼Œç”¨æˆ·è¾“å…¥çš„ä¿¡æ¯å°†ä¸ä¼šè¢«ä¿ç•™ï¼Œè€Œæ˜¯è¢«æ›´æ–°ä¸ºAIçš„å›å¤ï¼Œå› ä¸ºæ˜¯`update`æ¨¡å¼

**å¯¹æ¯”**

```python
# Start conversation, again
config = {"configurable": {"thread_id": "2"}}

# Start conversation
input_message = HumanMessage(content="hi! I'm Lance")
for event in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    for m in event['messages']:
        m.pretty_print()
    print("---"*25)
```

```python
================================ Human Message =================================

hi! I'm Lance
---------------------------------------------------------------------------
================================ Human Message =================================

hi! I'm Lance
================================== Ai Message ==================================

Hi Lance! How can I assist you today?
---------------------------------------------------------------------------
```



## Streaming tokens

æˆ‘ä»¬åœ¨æµå¼ä¼ è¾“æ—¶å¾€å¾€ä¸æ­¢å¸Œæœ›è·å¾—å›¾çŠ¶æ€ä¿¡æ¯ï¼Œä¾‹å¦‚å¯¹äºèŠå¤©æ¨¡å‹çš„é€šè¯è€Œè¨€ï¼Œé€šå¸¸ä¼šå®æ—¶ä¼ è¾“ç”Ÿæˆçš„ä»¤ç‰Œã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `.astream_events` æ–¹æ³•æ¥å®ç°è¿™ä¸€åŠŸèƒ½ï¼Œè¯¥æ–¹æ³•ä¼šåœ¨èŠ‚ç‚¹å†…éƒ¨å®æ—¶å›ä¼ äº‹ä»¶ï¼

æ¯ä¸ªäº‹ä»¶éƒ½æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«å‡ ä¸ªé”®å€¼å¯¹ï¼š

- eventï¼šè¿™æ˜¯æ­£åœ¨è¢«å‘å‡ºçš„æ­¤ç±»äº‹ä»¶ã€‚
- nameï¼šè¿™æ˜¯è¯¥äº‹ä»¶çš„åç§°ã€‚
- dataï¼šè¿™æ˜¯ä¸è¯¥äº‹ä»¶ç›¸å…³è”çš„æ•°æ®ã€‚
- metadataï¼šåŒ…å« langgraph_nodeï¼Œå³å‘å‡ºè¯¥äº‹ä»¶çš„èŠ‚ç‚¹ã€‚

```python
config = {"configurable": {"thread_id": "3"}}
input_message = HumanMessage(content="Tell me about the 49ers NFL team")
async for event in graph.astream_events({"messages": [input_message]}, config, version="v2"):
    print(f"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}")
```

```
Node: . Type: on_chain_start. Name: LangGraph
Node: __start__. Type: on_chain_start. Name: __start__
Node: __start__. Type: on_chain_end. Name: __start__
Node: conversation. Type: on_chain_start. Name: conversation
Node: conversation. Type: on_chat_model_start. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI

..........

Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_end. Name: ChatOpenAI
Node: conversation. Type: on_chain_start. Name: ChannelWrite<conversation,messages,summary>
Node: conversation. Type: on_chain_end. Name: ChannelWrite<conversation,messages,summary>
Node: conversation. Type: on_chain_start. Name: should_continue
Node: conversation. Type: on_chain_end. Name: should_continue
Node: conversation. Type: on_chain_stream. Name: conversation
Node: conversation. Type: on_chain_end. Name: conversation
Node: . Type: on_chain_stream. Name: LangGraph
Node: . Type: on_chain_end. Name: LangGraph
```

æ ¸å¿ƒè¦ç‚¹åœ¨äºï¼Œæ‚¨å›¾ä¸­çš„èŠå¤©æ¨¡å‹ç”Ÿæˆçš„ä»¤ç‰Œå…·æœ‰ `on_chat_model_stream` ç±»å‹ã€‚
æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `event['metadata']['langgraph_node']`æ¥é€‰æ‹©è¦è¿›è¡Œæµå¼ä¼ è¾“çš„èŠ‚ç‚¹ã€‚
å¹¶ä¸”æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `event['data']`æ¥è·å–æ¯ä¸ªäº‹ä»¶çš„å®é™…æ•°æ®ï¼Œè€Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥æ•°æ®æ˜¯ä¸€ä¸ª `AIMessageChunk` ç±»å‹çš„å¯¹è±¡ã€‚

```python
node_to_stream = 'conversation'
config = {"configurable": {"thread_id": "4"}}
input_message = HumanMessage(content="Tell me about the 49ers NFL team")
async for event in graph.astream_events({"messages": [input_message]}, config, version="v2"):
    # Get chat model tokens from a particular node 
    if event["event"] == "on_chat_model_stream" and event['metadata'].get('langgraph_node','') == node_to_stream:
        print(event["data"])
```

```
{'chunk': AIMessageChunk(content='', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content='The', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content=' San', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content=' Francisco', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content=' ', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content='49', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content='ers', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content=' are', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content=' a', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
{'chunk': AIMessageChunk(content=' professional', id='run-b76ec3b8-9c45-42fe-b321-4ec3a69c185c')}
.......

```

åªéœ€ä½¿ç”¨ chunk key æ¥è·å– AIMessageChunk å³å¯ã€‚

```python
config = {"configurable": {"thread_id": "5"}}
input_message = HumanMessage(content="Tell me about the 49ers NFL team")
async for event in graph.astream_events({"messages": [input_message]}, config, version="v2"):
    # Get chat model tokens from a particular node 
    if event["event"] == "on_chat_model_stream" and event['metadata'].get('langgraph_node','') == node_to_stream:
        data = event["data"]
        print(data["chunk"].content, end="|")
```

```
|The| San| Francisco| |49|ers| are| a| professional| American| football| team| based| in| the| San| Francisco| Bay| Area|.| They| compete| in| the| National| Football| League| (|NFL|)| as| a| member| of| the| league|'s| National| Football| Conference| (|N|FC|)| West| division|.| Here| are| some| key| points| about| the| team|:

|###| History|
|-| **|Founded|:**| |194|6| as| a| charter| member| of| the| All|-Amer|ica| Football| Conference| (|AA|FC|)| and| joined| the| NFL| in| |194|9| when| the| leagues| merged|.
|-| **|Team| Name|:**| The| name| "|49|ers|"| is| a| reference| to| the| prospect|ors| who| arrived| in| Northern| California| during| the| |184|9| Gold| Rush|.
```



# Breakpoints æ–­ç‚¹

å¯¹äº `human-in-the-loop` ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›åœ¨è¿è¡Œè¿‡ç¨‹ä¸­æŸ¥çœ‹å…¶graphè¾“å‡ºã€‚

1.  `Approval` - æˆ‘ä»¬å¯ä»¥ä¸­æ–­æˆ‘ä»¬çš„ä»£ç†ï¼Œå‘ç”¨æˆ·å‘ˆç°å½“å‰çŠ¶æ€ï¼Œå¹¶å…è®¸ç”¨æˆ·æ¥å—ä¸€ä¸ªæ“ä½œã€‚

2) `Debugging` - æˆ‘ä»¬å¯ä»¥å›æº¯å›¾ä»¥é‡ç°æˆ–é¿å…é—®é¢˜

3. `Editing` - æ‚¨å¯ä»¥ä¿®æ”¹çŠ¶æ€

## è®¾ç½®æ–­ç‚¹
æœ‰ä¸¤ä¸ªåœ°æ–¹å¯ä»¥è®¾ç½®æ–­ç‚¹ï¼š

* é€šè¿‡åœ¨ç¼–è¯‘æ—¶æˆ–è¿è¡Œæ—¶è®¾ç½®æ–­ç‚¹ï¼Œåœ¨èŠ‚ç‚¹æ‰§è¡Œä¹‹å‰æˆ–ä¹‹åæ‰§è¡Œã€‚æˆ‘ä»¬ç§°è¿™äº›ä¸º**é™æ€æ–­ç‚¹**ã€‚
* åœ¨èŠ‚ç‚¹å†…éƒ¨ä½¿ç”¨NodeInterruptå¼‚å¸¸ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º**åŠ¨æ€æ–­ç‚¹**ã€‚

æœ¬èŠ‚ä»‹ç»**é™æ€æ–­ç‚¹**

## é™æ€æ–­ç‚¹
é™æ€æ–­ç‚¹åœ¨èŠ‚ç‚¹æ‰§è¡Œä¹‹å‰æˆ–ä¹‹åè§¦å‘ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨**ç¼–è¯‘æ—¶**æˆ–**è¿è¡Œæ—¶**æŒ‡å®š`interrupt_before`å’Œ`interrupt_after`æ¥è®¾ç½®é™æ€æ–­ç‚¹ã€‚

å¦‚æœæ‚¨æƒ³æ¯æ¬¡å•æ­¥æ‰§è¡Œä¸€ä¸ªèŠ‚ç‚¹çš„å›¾ï¼Œæˆ–è€…æƒ³åœ¨ç‰¹å®šèŠ‚ç‚¹æš‚åœå›¾çš„æ‰§è¡Œï¼Œé™æ€æ–­ç‚¹å¯¹äºè°ƒè¯•ç‰¹åˆ«æœ‰ç”¨ã€‚

ç¼–è¯‘æ—¶

```python
graph = graph_builder.compile( 
    interrupt_before=["node_a"], 
    interrupt_after=["node_b", "node_c"], 
    checkpointer=checkpointer, 
)

config = {
    "configurable": {
        "thread_id": "some_thread"
    }
}

# Run the graph until the breakpoint
graph.invoke(inputs, config=thread_config) 

# Resume the graph
graph.invoke(None, config=thread_config)
```

è¿è¡Œæ—¶

```python
graph.invoke( 
    inputs, 
    interrupt_before=["node_a"], 
    interrupt_after=["node_b", "node_c"] 
    config={
        "configurable": {"thread_id": "some_thread"}
    }, 
)

config = {
    "configurable": {
        "thread_id": "some_thread"
    }
}

# Run the graph until the breakpoint
graph.invoke(inputs, config=config) 

# Resume the graph
graph.invoke(None, config=config)
```

## äººå·¥å®¡æ‰¹çš„æ–­ç‚¹

- è®©æˆ‘ä»¬é‡æ–°æ€è€ƒä¸€ä¸‹åœ¨æ¨¡å— 1 ä¸­æˆ‘ä»¬æ‰€ç ”ç©¶çš„é‚£ä¸ªç®€å•çš„æ™ºèƒ½ä½“ã€‚
- å‡è®¾æˆ‘ä»¬å…³æ³¨çš„æ˜¯å·¥å…·ä½¿ç”¨ï¼šæˆ‘ä»¬å¸Œæœ›æ‰¹å‡†è¯¥æ™ºèƒ½ä½“ä½¿ç”¨å…¶ä»»ä½•å·¥å…·ã€‚
- æˆ‘ä»¬æ‰€éœ€è¦åšçš„å°±æ˜¯ç®€å•åœ°ç”¨ `interrupt_before=["tools"]` ç¼–è¯‘å›¾å½¢ï¼Œå…¶ä¸­ `tools` æ˜¯æˆ‘ä»¬çš„å·¥å…·èŠ‚ç‚¹ã€‚
- è¿™æ„å‘³ç€æ‰§è¡Œå°†åœ¨æ‰§è¡Œå·¥å…·è°ƒç”¨çš„èŠ‚ç‚¹ `tools` ä¹‹å‰ä¸­æ–­ã€‚

module3/breakpoint.py

```python
from langchain_openai import ChatOpenAI

def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

# This will be a tool
def add(a: int, b: int) -> int:
    """Adds a and b.

    Args:
        a: first int
        b: second int
    """
    return a + b

def divide(a: int, b: int) -> float:
    """Divide a by b.

    Args:
        a: first int
        b: second int
    """
    return a / b

tools = [add, multiply, divide]
llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools(tools)

from IPython.display import Image, display

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition, ToolNode

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

# System message
sys_msg = SystemMessage(content="You are a helpful assistant tasked with performing arithmetic on a set of inputs.")

# Node
def assistant(state: MessagesState):
   return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

# Graph
builder = StateGraph(MessagesState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine the control flow
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "assistant")

memory = MemorySaver()
graph = builder.compile(interrupt_before=["tools"], checkpointer=memory)

# Show
display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```

![æˆªå±2025-04-30 10.05.04.png](/assets/images/æˆªå±2025-04-30%2010.05.04.png)

```python
# Input
initial_input = {"messages": HumanMessage(content="Multiply 2 and 3")}

# Thread
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

```
================================ Human Message =================================

Multiply 2 and 3
================================== Ai Message ==================================
Tool Calls:
  multiply (call_oFkGpnO8CuwW9A1rk49nqBpY)
 Call ID: call_oFkGpnO8CuwW9A1rk49nqBpY
  Args:
    a: 2
    b: 3
```

æˆ‘ä»¬å¯ä»¥è·å–çŠ¶æ€å¹¶æŸ¥çœ‹ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ¥è°ƒç”¨ã€‚è¿™æ˜¯ä¸€ç§å¾ˆå¥½çš„æ–¹å¼æ¥è¡¨æ˜å›¾å·²è¢«ä¸­æ–­ã€‚

```python
state = graph.get_state(thread)
state.next
```

```
('tools',)
```

**ç°åœ¨ï¼Œæˆ‘ä»¬è¦æ¥ä»‹ç»ä¸€ä¸ªä¸é”™çš„çªé—¨ã€‚å½“æˆ‘ä»¬ä½¿ç”¨ None æ¥è°ƒç”¨è¯¥å›¾æ—¶ï¼Œå®ƒå°†ä»ä¸Šä¸€æ¬¡çš„çŠ¶æ€æ£€æŸ¥ç‚¹ç»§ç»­è¿è¡Œï¼**


![æˆªå±2025-04-30 10.10.09.png](/assets/images/æˆªå±2025-04-30%2010.10.09.png)

ä¸ºäº†æ¸…æ™°èµ·è§ï¼ŒLangGraph å°†é‡æ–°å‘é€å½“å‰çŠ¶æ€ï¼Œå…¶ä¸­åŒ…å«å¸¦æœ‰å·¥å…·è°ƒç”¨çš„ AIMessageã€‚
ç„¶åå®ƒå°†æŒ‰ç…§å›¾ä¸­çš„æ­¥éª¤æ‰§è¡Œä¸‹å»ï¼Œè¿™äº›æ­¥éª¤å§‹äºå·¥å…·èŠ‚ç‚¹ï¼ˆtoolsï¼‰ã€‚
æˆ‘ä»¬çœ‹åˆ°ï¼Œå·¥å…·èŠ‚ç‚¹æ˜¯é€šè¿‡è¿™ä¸ªå·¥å…·è°ƒç”¨æŒ‡ä»¤æ¥è¿è¡Œçš„ï¼Œå¹¶ä¸”å®ƒä¼šè¢«ä¼ é€’å›èŠå¤©æ¨¡å‹ï¼Œä»¥ä¾›æˆ‘ä»¬å¾—å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

```python
for event in graph.stream(None, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

```
================================== Ai Message ==================================
Tool Calls:
  multiply (call_oFkGpnO8CuwW9A1rk49nqBpY)
 Call ID: call_oFkGpnO8CuwW9A1rk49nqBpY
  Args:
    a: 2
    b: 3
================================= Tool Message =================================
Name: multiply

6
================================== Ai Message ==================================

The result of multiplying 2 and 3 is 6.
```

**ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†è¿™äº›ä¸æ¥å—ç”¨æˆ·è¾“å…¥çš„ç‰¹å®šç”¨æˆ·æ‰¹å‡†æ­¥éª¤ç»“åˆåœ¨ä¸€èµ·ã€‚**

```python
# Input
initial_input = {"messages": HumanMessage(content="Multiply 2 and 3")}

# Thread
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    message = event['messages'][-1]
    print(f"Message type: {type(message)}")
    print(f"Message content: {message.content}")
    message.pretty_print()

# Get user feedback
user_approval = input("Do you want to call the tool? (yes/no): ")

# Check approval
if user_approval.lower() == "yes":
    # If approved, continue the graph execution
    for event in graph.stream(None, thread, stream_mode="values"):
        event['messages'][-1].pretty_print()
else:
    print("Operation cancelled by user.")    
```

* å½“graphæ‰§è¡Œåˆ°éœ€è¦è°ƒç”¨å·¥å…·æ—¶ï¼ˆæ¯”å¦‚è¿™é‡Œæ‰§è¡Œä¹˜æ³•æ“ä½œï¼‰ï¼Œå®ƒä¼šæš‚åœæ‰§è¡Œï¼ˆé€šè¿‡interrupt_before=["tools"]å®ç°ï¼‰
* æ­¤æ—¶ï¼Œä»£ç ä¼šé€šè¿‡input()å‡½æ•°è·å–ç”¨æˆ·çš„ç¡®è®¤ï¼š`user_approval = input("Do you want to call the tool? (yes/no): ")`

* æŸ¥çœ‹inputå‡½æ•°çš„å®šä¹‰ï¼Œå¯ä»¥çœ‹åˆ°å®ƒå°†ä¼šä»æ ‡å‡†è¾“å…¥è·å¾—ä¿¡æ¯

  ```python
  (function) def input(
      prompt: object = "",
      /
  ) -> str
  Read a string from standard input. The trailing newline is stripped.
  
  The prompt string, if given, is printed to standard output without a trailing newline before reading input.
  
  If the user hits EOF (*nix: Ctrl-D, Windows: Ctrl-Z+Return), raise EOFError. On *nix systems, readline is used if available.
  ```

è¿è¡Œæ—¶ï¼Œè¾“å‡º

```
Message type: <class 'langchain_core.messages.human.HumanMessage'>
Message content: Multiply 2 and 3
================================ Human Message =================================

Multiply 2 and 3
Message type: <class 'langchain_core.messages.ai.AIMessage'>
Message content: 
================================== Ai Message ==================================
Tool Calls:
  multiply (call_cd69029ef57f43f2b149f7)
 Call ID: call_cd69029ef57f43f2b149f7
  Args:
    a: 2
    b: 3
Do you want to call the tool? (yes/no): 
```

æ­¤æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è¾“å…¥yesï¼Œnoï¼Œå½±å“ç¨‹åºè¿è¡Œç»“æœ

```
Do you want to call the tool? (yes/no): yes
================================== Ai Message ==================================
Tool Calls:
  multiply (call_cd69029ef57f43f2b149f7)
 Call ID: call_cd69029ef57f43f2b149f7
  Args:
    a: 2
    b: 3
================================= Tool Message =================================
Name: multiply

6
================================== Ai Message ==================================

The product of 2 and 3 is 6.
```



# Editing graph state

## çŠ¶æ€ç¼–è¾‘

æˆ‘ä»¬åˆ©ç”¨å®ƒä»¬æ¥ä¸­æ–­å›¾å½¢æµç¨‹ï¼Œå¹¶åœ¨æ‰§è¡Œä¸‹ä¸€ä¸ªèŠ‚ç‚¹ä¹‹å‰ç­‰å¾…ç”¨æˆ·æ‰¹å‡†ã€‚åŒæ—¶ï¼Œæ–­ç‚¹ä¹Ÿæ˜¯[ä¿®æ”¹å›¾å½¢çŠ¶æ€çš„æœºä¼š](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/)ã€‚
è®©æˆ‘ä»¬åœ¨è¾…åŠ©èŠ‚ç‚¹ä¹‹å‰ç»™æˆ‘ä»¬çš„ä»£ç†è®¾ç½®ä¸€ä¸ªæ–­ç‚¹ã€‚

module3/editgraphstate.py

```python
from langchain_openai import ChatOpenAI

def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

# This will be a tool
def add(a: int, b: int) -> int:
    """Adds a and b.

    Args:
        a: first int
        b: second int
    """
    return a + b

def divide(a: int, b: int) -> float:
    """Divide a by b.

    Args:
        a: first int
        b: second int
    """
    return a / b

tools = [add, multiply, divide]
llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools(tools)

from IPython.display import Image, display

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition, ToolNode

from langchain_core.messages import HumanMessage, SystemMessage

# System message
sys_msg = SystemMessage(content="You are a helpful assistant tasked with performing arithmetic on a set of inputs.")

# Node
def assistant(state: MessagesState):
   return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

# Graph
builder = StateGraph(MessagesState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine the control flow
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "assistant")

memory = MemorySaver()
graph = builder.compile(interrupt_before=["assistant"], checkpointer=memory)

# Show
display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```

![æˆªå±2025-04-30 10.52.36.png](/assets/images/æˆªå±2025-04-30%2010.52.36.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨èŠå¤©æ¨¡å‹ä½œå‡ºå›åº”ä¹‹å‰ï¼Œå›¾å°±å·²ç»ä¸­æ–­äº†ã€‚

```python
# Input
initial_input = {"messages": "Multiply 2 and 3"}

# Thread
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

```
================================ Human Message =================================

Multiply 2 and 3
```

```
state = graph.get_state(thread)
state
```

```python
StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', id='e7edcaba-bfed-4113-a85b-25cc39d6b5a7')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a412-5b2d-601a-8000-4af760ea1d0d'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}}, created_at='2024-09-03T22:09:10.966883+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a412-5b28-6ace-bfff-55d7a2c719ae'}}, tasks=(PregelTask(id='dbee122a-db69-51a7-b05b-a21fab160696', name='assistant', error=None, interrupts=(), state=None),))
```

- ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥æ‰§è¡ŒçŠ¶æ€æ›´æ–°æ“ä½œã€‚

- è¯·è®°ä½ï¼Œå¯¹æ¶ˆæ¯é”®çš„æ›´æ–°å°†ä¼šä½¿ç”¨ `add_messages` reducerï¼š
- å¦‚æœæˆ‘ä»¬æƒ³è¦è¦†ç›–ç°æœ‰çš„æ¶ˆæ¯å†…å®¹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥æä¾›æ¶ˆæ¯ IDã€‚
- å¦‚æœæˆ‘ä»¬åªæ˜¯æƒ³åœ¨æˆ‘ä»¬çš„æ¶ˆæ¯åˆ—è¡¨ä¸­è¿½åŠ ä¸€æ¡æ–°æ¶ˆæ¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä¼ é€’ä¸€æ¡æœªæŒ‡å®šæ¶ˆæ¯ ID çš„æ¶ˆæ¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```python
graph.update_state(
    thread,
    {"messages": [HumanMessage(content="No, actually multiply 3 and 3!")]},
)
```

```
{'configurable': {'thread_id': '1',
  'checkpoint_ns': '',
  'checkpoint_id': '1ef6a414-f419-6182-8001-b9e899eca7e5'}}
```

æˆ‘ä»¬è°ƒç”¨äº† update_state å‡½æ•°ï¼Œå¹¶ä¼ å…¥äº†ä¸€æ¡æ–°çš„æ¶ˆæ¯ã€‚
add_messages è¿™ä¸ª reducer å°†å…¶é™„åŠ åˆ°æˆ‘ä»¬çš„çŠ¶æ€é”® messages ä¸Šã€‚

```python
new_state = graph.get_state(thread).values
for m in new_state['messages']:
    m.pretty_print()
```

```python
================================ Human Message =================================

Multiply 2 and 3
================================ Human Message =================================

No, actually multiply 3 and 3!
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç»§ç»­ä½¿ç”¨æˆ‘ä»¬çš„ä»£ç†ç¨‹åºï¼Œåªéœ€ä¼ é€’ None å¹¶å…è®¸å…¶ä»å½“å‰çŠ¶æ€ç»§ç»­æ‰§è¡Œå³å¯ã€‚
æˆ‘ä»¬å‘å‡ºå½“å‰èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œç„¶åç»§ç»­æ‰§è¡Œå…¶ä½™èŠ‚ç‚¹ã€‚

```python
for event in graph.stream(None, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

```
================================ Human Message =================================

No, actually multiply 3 and 3!
================================== Ai Message ==================================
Tool Calls:
  multiply (call_Mbu8MfA0krQh8rkZZALYiQMk)
 Call ID: call_Mbu8MfA0krQh8rkZZALYiQMk
  Args:
    a: 3
    b: 3
================================= Tool Message =================================
Name: multiply

9
```

ç°åœ¨ï¼Œæˆ‘ä»¬åˆå›åˆ°äº†`assistant`ç¨‹åºé‚£é‡Œï¼Œå®ƒè®¾ç½®äº†æˆ‘ä»¬çš„æ–­ç‚¹ã€‚
æˆ‘ä»¬å¯ä»¥å†æ¬¡ä¼ å…¥ None æ¥ç»§ç»­æ‰§è¡Œã€‚

```python
for event in graph.stream(None, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

```
================================= Tool Message =================================
Name: multiply

9
================================== Ai Message ==================================

3 multiplied by 3 equals 9.
```

## ç­‰å¾…ç”¨æˆ·è¾“å…¥

å¾ˆæ˜æ˜¾ï¼Œåœ¨é‡åˆ°æ–­ç‚¹æ—¶æˆ‘ä»¬å¯ä»¥ç¼–è¾‘æˆ‘ä»¬çš„ä»£ç†çŠ¶æ€ã€‚é‚£ä¹ˆï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦å…è®¸äººä¸ºåé¦ˆæ¥æ‰§è¡Œè¿™ç§çŠ¶æ€æ›´æ–°çš„è¯ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ

* æˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªnodeèŠ‚ç‚¹ï¼Œç”¨ä½œæˆ‘ä»¬ä»£ç†ç³»ç»Ÿä¸­[æ¥æ”¶äººç±»åé¦ˆçš„æš‚å­˜ä½ç½®](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/#setup)ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è¿™ä¸ªèŠ‚ç‚¹ç›´æ¥å¯¹ä»£ç†çš„çŠ¶æ€è¿›è¡Œä¿®æ”¹æˆ–è°ƒæ•´ï¼Œè€Œä¸æ˜¯é€šè¿‡å…¶ä»–é—´æ¥çš„æ–¹å¼ã€‚è¿™ä¸ª`human_feedback`èŠ‚ç‚¹å…è®¸ç”¨æˆ·ç›´æ¥å‘çŠ¶æ€æ·»åŠ åé¦ˆã€‚
* æˆ‘ä»¬åœ¨`human_feedback`èŠ‚ç‚¹çš„`interrupt_before`ä¸­æ–­ç‚¹è®¾ç½®ä¸­æ–­æ¡ä»¶ã€‚æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ª`checkpointer`ï¼Œç”¨äºä¿å­˜ä»è¯¥èŠ‚ç‚¹ä¹‹å‰åˆ°æ­¤èŠ‚ç‚¹ä¸ºæ­¢çš„å›¾çš„çŠ¶æ€ã€‚è¿™é‡Œçš„æ„æ€æ˜¯ï¼Œåœ¨äººç±»åé¦ˆèŠ‚ç‚¹ä¹‹å‰ï¼Œä¿å­˜æ•´ä¸ªç³»ç»Ÿï¼ˆå›¾ï¼‰çš„çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶å¯ä»¥æ¢å¤åˆ°è¿™ä¸ªçŠ¶æ€ï¼Œæˆ–è€…å¯¹çŠ¶æ€è¿›è¡Œåˆ†æå’Œä¿®æ”¹ã€‚

module3/editgraphstate2.py

```python
# System message
sys_msg = SystemMessage(content="You are a helpful assistant tasked with performing arithmetic on a set of inputs.")

# no-op node that should be interrupted on
def human_feedback(state: MessagesState):
    pass

# Assistant node
def assistant(state: MessagesState):
   return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

# Graph
builder = StateGraph(MessagesState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))
builder.add_node("human_feedback", human_feedback)

# Define edges: these determine the control flow
builder.add_edge(START, "human_feedback")
builder.add_edge("human_feedback", "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "human_feedback")

memory = MemorySaver()
graph = builder.compile(interrupt_before=["human_feedback"], checkpointer=memory)
display(Image(graph.get_graph().draw_mermaid_png()))
```

![æˆªå±2025-04-30 11.38.59.png](/assets/images/æˆªå±2025-04-30%2011.38.59.png)

**human_feedback èŠ‚ç‚¹çš„ä½œç”¨**ï¼š

- human_feedback æ˜¯ä¸€ä¸ªä¸­æ–­ç‚¹ï¼Œç”¨äºæš‚åœå›¾çš„æ‰§è¡Œï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥ã€‚
- å®ƒæœ¬èº«æ˜¯ä¸€ä¸ª pass èŠ‚ç‚¹ï¼Œä¸ä¼šå¯¹çŠ¶æ€è¿›è¡Œä»»ä½•å®é™…çš„å¤„ç†ï¼Œåªæ˜¯ä½œä¸ºä¸€ä¸ªæ ‡è®°ç‚¹

**graph.update_state çš„ä½œç”¨**ï¼š

- å½“è°ƒç”¨ graph.update_state æ—¶ï¼Œä½ å®é™…ä¸Šæ˜¯åœ¨æ‰‹åŠ¨æ›´æ–°å›¾çš„çŠ¶æ€ã€‚
- é€šè¿‡æŒ‡å®š as_node="human_feedback"ï¼Œä½ å‘Šè¯‰ç³»ç»Ÿè¿™æ¬¡çŠ¶æ€æ›´æ–°æ˜¯é€šè¿‡ human_feedback èŠ‚ç‚¹è¿›è¡Œçš„ã€‚
- è¿™ä¸ªæ“ä½œä¼šå°†ç”¨æˆ·è¾“å…¥çš„å†…å®¹ï¼ˆuser_inputï¼‰æ’å…¥åˆ°å›¾çš„çŠ¶æ€ä¸­ï¼Œå¹¶ä¸”æ ‡è®°ä¸º human_feedback èŠ‚ç‚¹çš„è¾“å‡ºã€‚

```py
# Input
initial_input = {"messages": "Multiply 2 and 3"}

# Thread
thread = {"configurable": {"thread_id": "5"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    event["messages"][-1].pretty_print()
    
# Get user input
user_input = input("Tell me how you want to update the state: ")

# We now update the state as if we are the human_feedback node
graph.update_state(thread, {"messages": user_input}, as_node="human_feedback")

# Continue the graph execution
for event in graph.stream(None, thread, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

```
# è¾“å…¥ï¼šno, multiply 3 and 3
================================ Human Message =================================

Multiply 2 and 3
Tell me how you want to update the state: no, multiply 3 and 3
================================ Human Message =================================

no, multiply 3 and 3
================================== Ai Message ==================================
Tool Calls:
  multiply (call_864833cafb1c4cd9b48f2d)
 Call ID: call_864833cafb1c4cd9b48f2d
  Args:
    a: 3
    b: 3
================================= Tool Message =================================
Name: multiply

9
```

ç»§ç»­

```python
# Continue the graph execution
for event in graph.stream(None, thread, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

```
================================= Tool Message =================================
Name: multiply

9
================================== Ai Message ==================================

The result of multiplying 3 and 3 is 9.
```

è¿™é‡Œæä¸ªç–‘é—®ï¼Œä¸ºä»€ä¹ˆç¬¬äºŒæ¬¡æ‰§è¡Œä¸ä¼šè§¦å‘ä¸­æ–­ï¼Ÿ
- åœ¨ç¬¬ä¸€æ¬¡æ‰§è¡Œ graph.stream æ—¶ï¼Œå›¾åœ¨ human_feedback èŠ‚ç‚¹å¤„ä¸­æ–­ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥ã€‚
- å½“ä½ è°ƒç”¨ graph.update_state å¹¶æŒ‡å®š as_node="human_feedback" æ—¶ï¼Œä½ å®é™…ä¸Šæ˜¯åœ¨æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥ï¼Œå¹¶å°†è¿™ä¸ªè¾“å…¥ä½œä¸º human_feedback èŠ‚ç‚¹çš„è¾“å‡ºã€‚
- å› æ­¤ï¼Œå½“ç¬¬äºŒæ¬¡æ‰§è¡Œ graph.stream æ—¶ï¼Œå›¾ä¼šä» human_feedback èŠ‚ç‚¹çš„è¾“å‡ºå¼€å§‹ç»§ç»­æ‰§è¡Œï¼Œè€Œä¸ä¼šå†æ¬¡ä¸­æ–­ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œå› ä¸º human_feedback èŠ‚ç‚¹å·²ç»è¢«â€œå¤„ç†â€è¿‡äº†ã€‚

# åŠ¨æ€æ–­ç‚¹

åœ¨å›¾å½¢ç¼–è¯‘è¿‡ç¨‹ä¸­ï¼Œå¼€å‘äººå‘˜ä¼šåœ¨ç‰¹å®šèŠ‚ç‚¹å¤„è®¾ç½®æ–­ç‚¹ï¼Œè¿™å±äºé™æ€æ–­ç‚¹ã€‚ä½†æ˜¯ï¼Œæœ‰æ—¶å€™å…è®¸å›¾å½¢åŠ¨æ€åœ°è‡ªè¡Œä¸­æ–­æ˜¯æœ‰å¸®åŠ©çš„ï¼Œè¿™å°±æ˜¯è¦ä»‹ç»çš„åŠ¨æ€æ–­ç‚¹`interrupt()`ã€‚

> æ³¨æ„ï¼šLangGraph 0.2.57ç‰ˆæœ¬èµ·æ¨èä½¿ç”¨ `interrupt()` å‡½æ•°æ¥å®ç°äººæœºäº¤äº’æ–­ç‚¹ï¼Œè€ç‰ˆæœ¬å¯ä»¥é€šè¿‡[è°ƒç”¨ NodeInterrupt æ¥å®ç°](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/dynamic_breakpoints/#run-the-graph-with-dynamic-interrupt)ã€‚

## interrupt

LangGraphä¸­çš„`interrupt`å‡½æ•°é€šè¿‡å°†å›¾æš‚åœåœ¨ç‰¹å®šèŠ‚ç‚¹ï¼Œå°†ä¿¡æ¯å‘ˆç°ç»™äººï¼Œç„¶åæ ¹æ®äººçš„è¾“å…¥ï¼ˆCommandå¯¹è±¡ï¼‰æ¢å¤å›¾æ¥å®ç°äººåœ¨å›è·¯å·¥ä½œæµã€‚å®ƒå¯¹äºå®¡æ‰¹ã€ç¼–è¾‘æˆ–æ”¶é›†é¢å¤–çš„ä¸Šä¸‹æ–‡ç­‰ä»»åŠ¡éå¸¸æœ‰ç”¨ã€‚

`interrupt`å±äºåŠ¨æ€æ–­ç‚¹ï¼š

1. **è¿è¡Œæ—¶å†³å®š**ï¼šå®ƒåœ¨èŠ‚ç‚¹æ‰§è¡Œè¿‡ç¨‹ä¸­æ ¹æ®ç¨‹åºé€»è¾‘åŠ¨æ€è°ƒç”¨ï¼Œè€Œä¸æ˜¯é¢„å…ˆé…ç½®
2. **æ¡ä»¶è§¦å‘**ï¼šå¯ä»¥åŸºäºçŠ¶æ€æˆ–å…¶ä»–æ¡ä»¶æ¥å†³å®šæ˜¯å¦è°ƒç”¨ `interrupt`
3. **çµæ´»æ§åˆ¶**ï¼šå¯ä»¥åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸­å¤šæ¬¡è°ƒç”¨ï¼Œå®ç°å¤æ‚çš„äº¤äº’é€»è¾‘

### `interrupt`å‡½æ•°å®šä¹‰

`interrupt`å‡½æ•°å®šä¹‰å¦‚ä¸‹æ‰€ç¤ºï¼Œå‡½æ•°æ¥å—ä¸€ä¸ª `value: Any` å‚æ•°ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥ä¼ é€’ä»»ä½• JSON å¯åºåˆ—åŒ–çš„å€¼ã€‚é”®åå¯ä»¥éšæ„è®¾å®šï¼Œå¯ä»¥ä¼ é€’å¤æ‚çš„åµŒå¥—ç»“æ„ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªæ™®é€šçš„stringã€‚

```python
def interrupt(value: Any) -> Any:
    """Interrupt the graph with a resumable exception from within a node.

    The `interrupt` function enables human-in-the-loop workflows by pausing graph
    execution and surfacing a value to the client. This value can communicate context
    or request input required to resume execution.

    In a given node, the first invocation of this function raises a `GraphInterrupt`
    exception, halting execution. The provided `value` is included with the exception
    and sent to the client executing the graph.

    A client resuming the graph must use the [`Command`][langgraph.types.Command]
    primitive to specify a value for the interrupt and continue execution.
    The graph resumes from the start of the node, **re-executing** all logic.

    If a node contains multiple `interrupt` calls, LangGraph matches resume values
    to interrupts based on their order in the node. This list of resume values
    is scoped to the specific task executing the node and is not shared across tasks.

    To use an `interrupt`, you must enable a checkpointer, as the feature relies
    on persisting the graph state.
```

### interrupt() ä¸ input() çš„åŒºåˆ«

interrupt() æ˜¯ LangGraph ä¸­ç”¨äºå®ç° **äººæœºäº¤äº’** çš„ä¸€ä¸ªå‡½æ•°ã€‚å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯ **æš‚åœå›¾çš„æ‰§è¡Œ**ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œç„¶åå°†ç”¨æˆ·è¾“å…¥ä½œä¸ºåé¦ˆç»§ç»­æ‰§è¡Œå›¾ã€‚

åœ¨ä¼ ç»Ÿçš„ç¼–ç¨‹ä¸­ï¼Œinput() æ˜¯ä¸€ä¸ªé˜»å¡å¼å‡½æ•°ï¼Œå®ƒä¼šç›´æ¥ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œç›´åˆ°ç”¨æˆ·æŒ‰ä¸‹å›è½¦é”®åæ‰ç»§ç»­æ‰§è¡Œä»£ç ã€‚ç„¶è€Œï¼Œinterrupt() çš„è®¾è®¡å¹¶ä¸æ˜¯ç›´æ¥é˜»å¡ç¨‹åºï¼Œè€Œæ˜¯é€šè¿‡ **ä¸­æ–­å›¾çš„æ‰§è¡Œ** æ¥ç­‰å¾…ç”¨æˆ·è¾“å…¥ã€‚

### `interrupt`ä¸`Command`ç»“åˆä½¿ç”¨

æ¥ä¸‹æ¥é€šè¿‡ä¸€ä¸ªå®Œæ•´ç¤ºä¾‹è§£é‡Š`interrupt`ä¸`Command`ç»“åˆä½¿ç”¨å®Œæˆä¸­æ–­è®¾è®¡ã€‚

module3/interrupt.py

```python
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

from langgraph.types import Command, interrupt
from langgraph.checkpoint.memory import MemorySaver
from IPython.display import Image, display

class State(TypedDict):
    input: str
    user_feedback: str

def step_1(state):
    print("---Step 1---")
    pass

def human_feedback(state):
    print("---human_feedback---")
    feedback = interrupt("Please provide feedback:")
    return {"user_feedback": feedback}

def step_3(state):
    print("---Step 3---")
    pass

builder = StateGraph(State)
builder.add_node("step_1", step_1)
builder.add_node("human_feedback", human_feedback)
builder.add_node("step_3", step_3)
builder.add_edge(START, "step_1")
builder.add_edge("step_1", "human_feedback")
builder.add_edge("human_feedback", "step_3")
builder.add_edge("step_3", END)

# Set up memory
memory = MemorySaver()

# Add
graph = builder.compile(checkpointer=memory)

# Input
initial_input = {"input": "hello world"}

# Thread
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="updates"):
    print(event)
    print("\n")

# Continue the graph execution
for event in graph.stream(
    Command(resume="go to step 3!"),
    thread,
    stream_mode="updates",
):
    print(event)
    print("\n")

graph.get_state(thread).values
```

![bc6f0bebebd09f4698372814bf62a134.png](/assets/images/bc6f0bebebd09f4698372814bf62a134.png)

- **ä¸­æ–­æ‰§è¡Œ**ï¼šå½“ interrupt() è¢«è°ƒç”¨æ—¶ï¼Œå®ƒä¼š **æš‚åœå½“å‰èŠ‚ç‚¹çš„æ‰§è¡Œ**ï¼Œå¹¶è¿”å›ä¸€ä¸ªä¸­æ–­ä¿¡å·ï¼ˆ__interrupt__ï¼‰ã€‚è¿™ä¸ªä¸­æ–­ä¿¡å·åŒ…å«äº†ä¸€ä¸ªæç¤ºä¿¡æ¯ï¼ˆå¦‚â€œPlease provide feedback:â€ï¼‰ï¼Œå‘Šè¯‰ç”¨æˆ·éœ€è¦æä¾›è¾“å…¥ã€‚
- **ä¿å­˜çŠ¶æ€**ï¼š
  - åœ¨ä¸­æ–­æ‰§è¡Œæ—¶ï¼ŒLangGraph ä¼šé€šè¿‡ **æ£€æŸ¥ç‚¹æœºåˆ¶**ï¼ˆå¦‚ MemorySaverï¼‰ä¿å­˜å½“å‰å›¾çš„çŠ¶æ€ã€‚
  - è¿™æ ·å¯ä»¥ç¡®ä¿åœ¨ç”¨æˆ·è¾“å…¥åï¼Œå›¾å¯ä»¥ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­æ‰§è¡Œã€‚
- **æ¢å¤æ‰§è¡Œ**ï¼š
  - ç”¨æˆ·è¾“å…¥åï¼Œé€šè¿‡è°ƒç”¨ Command(resume=<user_input>)ï¼Œå°†ç”¨æˆ·è¾“å…¥ä¼ é€’ç»™å›¾ã€‚
  - å›¾ä¼šæ¢å¤æ‰§è¡Œï¼Œå¹¶å°†ç”¨æˆ·è¾“å…¥ä½œä¸ºåé¦ˆä¼ é€’ç»™ä¸­æ–­çš„èŠ‚ç‚¹ã€‚

å…³é”®ä»£ç 

```python
def human_feedback(state):
    print("---human_feedback---")
    feedback = interrupt("Please provide feedback:")
    return {"user_feedback": feedback}
```

- åœ¨ human_feedback å‡½æ•°ä¸­ï¼Œinterrupt() è¢«è°ƒç”¨ï¼Œæ‰“å°æç¤ºä¿¡æ¯â€œPlease provide feedback:â€ã€‚
- æ­¤æ—¶ï¼Œå›¾çš„æ‰§è¡Œè¢«æš‚åœï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥ã€‚
- ç”¨æˆ·è¾“å…¥åï¼Œé€šè¿‡ Command(resume=<user_input>) æ¢å¤æ‰§è¡Œï¼Œå¹¶å°†è¾“å…¥å€¼èµ‹ç»™ user_feedbackã€‚

### interrupt() çš„æ ¸å¿ƒæ€æƒ³

interrupt() çš„æ ¸å¿ƒæ€æƒ³æ˜¯ **å°†ç”¨æˆ·è¾“å…¥çš„ç­‰å¾…è¿‡ç¨‹ä»å›¾çš„æ‰§è¡Œä¸­è§£è€¦**ã€‚å®ƒå¹¶ä¸æ˜¯ç›´æ¥é˜»å¡ç¨‹åºç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œè€Œæ˜¯é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°äº¤äº’ï¼š

1. **ä¸­æ–­æ‰§è¡Œ**ï¼šinterrupt() è¢«è°ƒç”¨æ—¶ï¼Œå›¾çš„æ‰§è¡Œè¢«æš‚åœï¼Œè¿”å›ä¸€ä¸ªä¸­æ–­ä¿¡å·ï¼Œæç¤ºéœ€è¦ç”¨æˆ·è¾“å…¥ã€‚
2. ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼šè¿™ä¸ªæ­¥éª¤å¹¶ä¸æ˜¯ç”±interrupt()ç›´æ¥å®Œæˆçš„ï¼Œè€Œæ˜¯ç”±å¤–éƒ¨æœºåˆ¶ï¼ˆå¦‚å‰ç«¯ç•Œé¢ã€API è°ƒç”¨ç­‰ï¼‰æ¥å®ç°ã€‚ç”¨æˆ·è¾“å…¥å¯ä»¥æ¥è‡ªä»»ä½•æ¸ é“ï¼Œæ¯”å¦‚ï¼š
   - ä¸€ä¸ªç½‘é¡µè¡¨å•çš„æäº¤ã€‚
   - ä¸€ä¸ªç§»åŠ¨åº”ç”¨çš„ç”¨æˆ·äº¤äº’ã€‚
   - ä¸€ä¸ª API æ¥å£æ¥æ”¶çš„ç”¨æˆ·è¾“å…¥ã€‚
   - ç”šè‡³å¯ä»¥æ˜¯ç¨‹åºè®¾è®¡è€…æ¨¡æ‹Ÿçš„è¾“å…¥ã€‚
3. **æ¢å¤æ‰§è¡Œ**ï¼šé€šè¿‡è°ƒç”¨ Command(resume=<user_input>)ï¼Œå°†ç”¨æˆ·è¾“å…¥ä¼ é€’ç»™å›¾ï¼Œæ¢å¤æ‰§è¡Œã€‚

### å…³é”®ç‚¹

- **çµæ´»æ€§**ï¼šinterrupt() å¹¶ä¸ç›´æ¥å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œè€Œæ˜¯é€šè¿‡å¤–éƒ¨æœºåˆ¶è·å–ç”¨æˆ·è¾“å…¥ã€‚è¿™æ„å‘³ç€å¼€å‘è€…å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©æœ€é€‚åˆçš„ç”¨æˆ·è¾“å…¥æ–¹å¼ã€‚
- **è§£è€¦**ï¼šç”¨æˆ·è¾“å…¥çš„è·å–å’Œå›¾çš„æ‰§è¡Œæ˜¯è§£è€¦çš„ã€‚å›¾çš„æ‰§è¡Œå¯ä»¥æš‚åœç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œè€Œç”¨æˆ·è¾“å…¥çš„è·å–å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹ã€ä»»ä½•æ—¶é—´å®Œæˆã€‚
- **æ¨¡æ‹Ÿè¾“å…¥**ï¼šCommand(resume=<user_input>) å…è®¸å¼€å‘è€…æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥ï¼Œè¿™æ„å‘³ç€å¯ä»¥åœ¨æµ‹è¯•ç¯å¢ƒä¸­è½»æ¾åœ°æµ‹è¯•å›¾çš„è¡Œä¸ºï¼Œè€Œä¸éœ€è¦çœŸæ­£çš„ç”¨æˆ·äº¤äº’ã€‚



## NodeInterruptï¼ˆä¸æ¨èä½¿ç”¨ï¼‰

è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå›¾ï¼Œåœ¨è¯¥å›¾ä¸­ï¼Œä¼šæ ¹æ®è¾“å…¥çš„é•¿åº¦æŠ›å‡º `NodeInterrupt` å¼‚å¸¸ã€‚

module3/NodeInterrupt.py

```python
from IPython.display import Image, display

from typing_extensions import TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.errors import NodeInterrupt
from langgraph.graph import START, END, StateGraph

class State(TypedDict):
    input: str

def step_1(state: State) -> State:
    print("---Step 1---")
    return state

def step_2(state: State) -> State:
    # Let's optionally raise a NodeInterrupt if the length of the input is longer than 5 characters
    if len(state['input']) > 5:
        raise NodeInterrupt(f"Received input that is longer than 5 characters: {state['input']}")
    
    print("---Step 2---")
    return state

def step_3(state: State) -> State:
    print("---Step 3---")
    return state

builder = StateGraph(State)
builder.add_node("step_1", step_1)
builder.add_node("step_2", step_2)
builder.add_node("step_3", step_3)
builder.add_edge(START, "step_1")
builder.add_edge("step_1", "step_2")
builder.add_edge("step_2", "step_3")
builder.add_edge("step_3", END)

# Set up memory
memory = MemorySaver()

# Compile the graph with memory
graph = builder.compile(checkpointer=memory)

# View
display(Image(graph.get_graph().draw_mermaid_png()))
```

![æˆªå±2025-04-30 14.37.06.png](/assets/images/æˆªå±2025-04-30%2014.37.06.png)

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªè¶…è¿‡ 5 ä¸ªå­—ç¬¦çš„è¾“å…¥æ¥è¿è¡Œè¿™ä¸ªå›¾ã€‚

```python
initial_input = {"input": "hello world"}
thread_config = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread_config, stream_mode="values"):
    print(event)
```

```python
{'input': 'hello world'}
---Step 1---
{'input': 'hello world'}
```

å¦‚æœæˆ‘ä»¬æ­¤æ—¶æŸ¥çœ‹å›¾çš„çŠ¶æ€ï¼Œæˆ‘ä»¬å°±ä¼šå¾—åˆ°æ¥ä¸‹æ¥è¦æ‰§è¡Œçš„èŠ‚ç‚¹é›†åˆï¼ˆstep_2ï¼‰ã€‚

```python
state = graph.get_state(thread_config)
print(state.next)
```

```
('step_2',)
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ`Interrupt` è¢«è®°å½•åˆ°äº†çŠ¶æ€ä¿¡æ¯ä¸­ã€‚

```python
print(state.tasks)
```

```
(PregelTask(id='6eb3910d-e231-5ba2-b25e-28ad575690bd', name='step_2', error=None, interrupts=(Interrupt(value='Received input that is longer than 5 characters: hello world', when='during'),), state=None),)
```

æˆ‘ä»¬å¯ä»¥å°è¯•ä»æ–­ç‚¹å¤„é‡æ–°ç»˜åˆ¶è¯¥å›¾ã€‚ä½†æ˜¯ï¼Œè¿™ä¸è¿‡æ˜¯åœ¨é‡å¤ä½¿ç”¨åŒä¸€ä¸ªèŠ‚ç‚¹ï¼é™¤éæ”¹å˜çŠ¶æ€ï¼Œå¦åˆ™æˆ‘ä»¬å°±ä¼šè¢«å›°åœ¨è¿™é‡Œã€‚

```python
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)
```

```
{'input': 'hello world'}
```

```
state = graph.get_state(thread_config)
print(state.next)
```

```
('step_2',)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–°çŠ¶æ€äº†ã€‚

```python
graph.update_state( thread_config, {"input": "hi"},)
```

```
{'configurable': {'thread_id': '1',
  'checkpoint_ns': '',
  'checkpoint_id': '1ef6a434-06cf-6f1e-8002-0ea6dc69e075'}}
```

```python
for event in graph.stream(None, thread_config, stream_mode="values"):
    print(event)
```

```python
{'input': 'hi'}
---Step 2---
{'input': 'hi'}
---Step 3---
{'input': 'hi'}
```

åœ¨æŸäº›éœ€è¦æŠ›å‡ºå¼‚å¸¸æ¥ä¸­æ–­æ‰§è¡Œçš„åœºæ™¯ä¸­ï¼Œ`NodeInterrupt` ä½œä¸ºå¼‚å¸¸ç±»å‹å¯èƒ½æ›´ç¬¦åˆä¼ ç»Ÿçš„é”™è¯¯å¤„ç†æ¨¡å¼ã€‚

# Time travel

åœ¨ä½¿ç”¨åŸºäºæ¨¡å‹åšå‡ºå†³ç­–çš„éç¡®å®šæ€§ç³»ç»Ÿï¼ˆä¾‹å¦‚ï¼Œç”± LLMs é©±åŠ¨çš„ä»£ç†ï¼‰æ—¶ï¼Œè¯¦ç»†æ£€æŸ¥å…¶å†³ç­–è¿‡ç¨‹å¯èƒ½ä¼šå¾ˆæœ‰ç”¨

1. ğŸ¤” **ç†è§£æ¨ç†**ï¼šåˆ†æå¯¼è‡´æˆåŠŸç»“æœçš„æ­¥éª¤ã€‚
2. ğŸ **è°ƒè¯•é”™è¯¯**ï¼šè¯†åˆ«é”™è¯¯å‘ç”Ÿçš„ä½ç½®å’ŒåŸå› ã€‚
3. ğŸ” **æ¢ç´¢æ›¿ä»£æ–¹æ¡ˆ**ï¼šæµ‹è¯•ä¸åŒçš„è·¯å¾„ä»¥å‘ç°æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å±•ç¤ºä¸€ä¸‹ LangGraph å¦‚ä½•æ”¯æŒè°ƒè¯•åŠŸèƒ½ï¼ŒåŒ…æ‹¬æŸ¥çœ‹å†å²ã€å›æ”¾ä»¥åŠä»è¿‡å¾€çŠ¶æ€è¿›è¡Œåˆ†æ”¯æ“ä½œç­‰æ“ä½œã€‚æˆ‘ä»¬æŠŠè¿™ç§ç°è±¡ç§°ä¸ºæ—¶é—´æ—…è¡Œã€‚

```python
from langchain_openai import ChatOpenAI

def multiply(a: int, b: int) -> int:
    """Multiply a and b.

    Args:
        a: first int
        b: second int
    """
    return a * b

# This will be a tool
def add(a: int, b: int) -> int:
    """Adds a and b.

    Args:
        a: first int
        b: second int
    """
    return a + b

def divide(a: int, b: int) -> float:
    """Divide a by b.

    Args:
        a: first int
        b: second int
    """
    return a / b

tools = [add, multiply, divide]
llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools(tools)

from IPython.display import Image, display

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import MessagesState
from langgraph.graph import START, END, StateGraph
from langgraph.prebuilt import tools_condition, ToolNode

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

# System message
sys_msg = SystemMessage(content="You are a helpful assistant tasked with performing arithmetic on a set of inputs.")

# Node
def assistant(state: MessagesState):
   return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])]}

# Graph
builder = StateGraph(MessagesState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine the control flow
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "assistant")

memory = MemorySaver()
graph = builder.compile(checkpointer=MemorySaver())

# Show
display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```



```py
# Input
initial_input = {"messages": HumanMessage(content="Multiply 2 and 3")}

# Thread
thread = {"configurable": {"thread_id": "1"}}

# Run the graph until the first interruption
for event in graph.stream(initial_input, thread, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

```
================================ Human Message =================================

Multiply 2 and 3
================================== Ai Message ==================================
Tool Calls:
  multiply (call_ikJxMpb777bKMYgmM3d9mYjW)
 Call ID: call_ikJxMpb777bKMYgmM3d9mYjW
  Args:
    a: 2
    b: 3
================================= Tool Message =================================
Name: multiply

6
================================== Ai Message ==================================

The result of multiplying 2 and 3 is 6.
```

## Browsing Historyæµè§ˆå†å²

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ get_state å‡½æ•°æ¥æŸ¥çœ‹ç»™å®šçº¿ç¨‹ ID å¯¹åº”çš„å›¾çš„å½“å‰çŠ¶æ€ï¼

```py
graph.get_state({'configurable': {'thread_id': '1'}})
```

```python
StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ikJxMpb777bKMYgmM3d9mYjW', 'function': {'arguments': '{"a":2,"b":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 131, 'total_tokens': 148}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bc24d334-8013-4f85-826f-e1ed69c86df0-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_ikJxMpb777bKMYgmM3d9mYjW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 17, 'total_tokens': 148}), ToolMessage(content='6', name='multiply', id='1012611a-30c5-4732-b789-8c455580c7b4', tool_call_id='call_ikJxMpb777bKMYgmM3d9mYjW'), AIMessage(content='The result of multiplying 2 and 3 is 6.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 156, 'total_tokens': 170}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-b46f3fed-ca3b-4e09-83f4-77ea5071e9bf-0', usage_metadata={'input_tokens': 156, 'output_tokens': 14, 'total_tokens': 170})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-ac9e-6024-8003-6fd8435c1d3b'}}, metadata={'source': 'loop', 'writes': {'assistant': {'messages': [AIMessage(content='The result of multiplying 2 and 3 is 6.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 156, 'total_tokens': 170}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-b46f3fed-ca3b-4e09-83f4-77ea5071e9bf-0', usage_metadata={'input_tokens': 156, 'output_tokens': 14, 'total_tokens': 170})]}}, 'step': 3, 'parents': {}}, created_at='2024-09-03T22:29:54.309727+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-a759-6d02-8002-f1da6393e1ab'}}, tasks=())
```

StateSnapshotæ˜¯LangGraphæŒä¹…æ€§æ¶æ„çš„ä¸€ä¸ªåŸºæœ¬ç»„ä»¶ã€‚å®ƒä¸checkpointersæºæ‰‹å·¥ä½œï¼Œä»¥æ”¯æŒé«˜çº§å·¥ä½œæµåŠŸèƒ½ï¼Œå¦‚ä¸­æ–­ã€çŠ¶æ€æ›´æ–°å’Œåˆ†æ”¯æ‰§è¡Œè·¯å¾„ã€‚å¿«ç…§åŒ…å«äº†ç†è§£å›¾ä¸­åˆ°ç›®å‰ä¸ºæ­¢å‘ç”Ÿäº†ä»€ä¹ˆä»¥åŠæ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆæ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ï¼Œè¿™ä½¿å¾—å®ƒå¯¹äºè°ƒè¯•ã€ç›‘æ§å’Œæ§åˆ¶å›¾çš„æ‰§è¡Œæµç¨‹è‡³å…³é‡è¦ã€‚

> StateSnapshotè¢«å®ç°ä¸ºä¸€ä¸ªåŒ…å«7ä¸ªå…³é”®å­—æ®µçš„å‘½åå…ƒç»„ï¼š
>
> `values`ï¼šåŒ…å«å›¾ä¸­æ‰€æœ‰çŠ¶æ€é€šé“çš„å½“å‰å€¼
> `next`ï¼šè®¡åˆ’åœ¨ä¸‹ä¸€æ­¥ä¸­æ‰§è¡Œçš„èŠ‚ç‚¹åç§°çš„å…ƒç»„
> `config`ï¼šç”¨äºè·å–æ­¤å¿«ç…§çš„RunnableConfig
> `metadata`ï¼šä¸æ£€æŸ¥ç‚¹ç›¸å…³çš„å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬æ­¥éª¤ä¿¡æ¯å’Œå†™
> `created_at`ï¼šå¿«ç…§åˆ›å»ºæ—¶é—´æˆ³
> `parent_config`ï¼šç”¨äºè·å–çˆ¶å¿«ç…§çš„é…ç½®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚
> `tasks`ï¼ˆä»»åŠ¡ï¼‰ï¼šPregelTaskå¯¹è±¡çš„å…ƒç»„ï¼Œè¡¨ç¤ºè¿™ä¸€æ­¥è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå¦‚æœä¹‹å‰å°è¯•è¿‡ï¼Œå¯èƒ½ä¼šåŒ…å«é”™è¯¯ä¿¡æ¯



æˆ‘ä»¬è¿˜å¯ä»¥æµè§ˆä»£ç†çš„çŠ¶æ€å†å²è®°å½•ã€‚`get_state_history` æ–¹æ³•è®©æˆ‘ä»¬èƒ½å¤Ÿè·å–æ‰€æœ‰å…ˆå‰æ­¥éª¤çš„çŠ¶æ€ã€‚

```py
all_states = [s for s in graph.get_state_history(thread)]
len(all_states)
```

5

ç¬¬ä¸€ä¸ªè¦ç´ å°±æ˜¯å½“å‰çŠ¶æ€ï¼Œæ­£å¦‚æˆ‘ä»¬ä» `get_state` å‡½æ•°ä¸­è·å–åˆ°çš„é‚£æ ·ã€‚

```
all_states[-2]
```

```python
StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-a003-6c74-8000-8a2d82b0d126'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}}, created_at='2024-09-03T22:29:52.988265+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-9ffe-6512-bfff-9e6d8dc24bba'}}, tasks=(PregelTask(id='ca669906-0c4f-5165-840d-7a6a3fce9fb9', name='assistant', error=None, interrupts=(), state=None),))
```

ä¸Šè¿°æ‰€æœ‰å†…å®¹æˆ‘ä»¬éƒ½èƒ½å¤Ÿå¯è§†åŒ–å‡ºæ¥ï¼š

![fdb72c07ddb6fb0570fc7c6de4c36e95.png](/assets/images/fdb72c07ddb6fb0570fc7c6de4c36e95.png)

## Replayingå›æ”¾

æˆ‘ä»¬å¯ä»¥ä»ä¹‹å‰çš„ä»»ä½•æ­¥éª¤é‡æ–°å¯åŠ¨æˆ‘ä»¬çš„ä»£ç†ç¨‹åºã€‚

![de6f010423cb95c57143d3658ce8e833.png](/assets/images/de6f010423cb95c57143d3658ce8e833.png)

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æ¥æ”¶äººç±»è¾“å…¥çš„é‚£ä¸€ç¯èŠ‚ï¼

```python
to_replay = all_states[-2]
```

```
StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-a003-6c74-8000-8a2d82b0d126'}}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}}, created_at='2024-09-03T22:29:52.988265+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-9ffe-6512-bfff-9e6d8dc24bba'}}, tasks=(PregelTask(id='ca669906-0c4f-5165-840d-7a6a3fce9fb9', name='assistant', error=None, interrupts=(), state=None),))
```

```python
to_replay.values
```

```
{'messages': [HumanMessage(content='Multiply 2 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d')]}
```

ä¸‹ä¸€ä¸ªè°ƒç”¨node


```python
to_replay.next
```


    ('assistant',)

æˆ‘ä»¬è¿˜è·å–åˆ°äº†é…ç½®ä¿¡æ¯ï¼Œå…¶ä¸­åŒ…å«äº† `checkpoint_id` ä»¥åŠ `thread_id` è¿™ä¸¤ä¸ªå‚æ•°ã€‚


```python
to_replay.config
```


    {'configurable': {'thread_id': '1',
      'checkpoint_ns': '',
      'checkpoint_id': '1ef6a440-a003-6c74-8000-8a2d82b0d126'}}

è¦ä»è¿™é‡Œé‡æ–°æ’­æ”¾ï¼Œæˆ‘ä»¬åªéœ€å°†é…ç½®ä¿¡æ¯ä¼ é€’ç»™ä»£ç†å³å¯ï¼è¯¥å›¾çŸ¥æ™“æ­¤æ£€æŸ¥ç‚¹å·²æ‰§è¡Œè¿‡ã€‚å®ƒå°±ä»è¿™ä¸ªæ£€æŸ¥ç‚¹é‡æ–°å¼€å§‹æ’­æ”¾ï¼


```python
for event in graph.stream(None, to_replay.config, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

    ================================ Human Message =================================
    
    Multiply 2 and 3
    ================================== Ai Message ==================================
    Tool Calls:
      multiply (call_SABfB57CnDkMu9HJeUE0mvJ9)
     Call ID: call_SABfB57CnDkMu9HJeUE0mvJ9
      Args:
        a: 2
        b: 3
    ================================= Tool Message =================================
    Name: multiply
    
    6
    ================================== Ai Message ==================================
    
    The result of multiplying 2 and 3 is 6.


ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨ä»£ç†é‡æ–°è¿è¡Œä¹‹åæˆ‘ä»¬çš„å½“å‰çŠ¶å†µã€‚

## Forking åˆ†æ”¯
è¦æ˜¯æˆ‘ä»¬æƒ³ä»åŒä¸€ä¸ªæ­¥éª¤å¼€å§‹æ“ä½œï¼Œä½†è¿™æ¬¡ä½¿ç”¨ä¸åŒçš„è¾“å…¥æ•°æ®ä¼šæ€æ ·å‘¢ï¼Ÿ

![0110f4bfea74e82b64cce58f38906133.png](/assets/images/0110f4bfea74e82b64cce58f38906133.png)

```python
to_fork = all_states[-2]
to_fork.values["messages"]
```


    [HumanMessage(content='Multiply 2 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d')]

æŸ¥çœ‹config


```python
to_fork.config
```


    {'configurable': {'thread_id': '1',
      'checkpoint_ns': '',
      'checkpoint_id': '1ef6a440-a003-6c74-8000-8a2d82b0d126'}}

è®©æˆ‘ä»¬åœ¨è¿™ä¸ªæ£€æŸ¥ç‚¹å¯¹çŠ¶æ€è¿›è¡Œä¿®æ”¹ã€‚
æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ `checkpoint_id` æ¥è¿è¡Œ `update_state` å‡½æ•°ã€‚
è¿˜è®°å¾—æˆ‘ä»¬åœ¨ `messages` ä¸Šé¢çš„reduceræ˜¯å¦‚ä½•è¿ä½œçš„å—ï¼š

- å¦‚æœæˆ‘ä»¬ä¸æä¾›æ¶ˆæ¯ IDï¼Œå®ƒå°±ä¼šè¿½åŠ ï¼ˆå†…å®¹ï¼‰ã€‚
- æˆ‘ä»¬æä¾›æ¶ˆæ¯ ID æ˜¯ä¸ºäº†è¦†ç›–è¯¥æ¶ˆæ¯å†…å®¹ï¼Œè€Œä¸æ˜¯å¯¹å…¶è¿›è¡Œè¿½åŠ ï¼ï¼ˆä»¥æ›´æ–°çŠ¶æ€ï¼‰ï¼

æ‰€ä»¥ï¼Œè¦è¦†ç›–è¿™æ¡æ¶ˆæ¯ï¼Œæˆ‘ä»¬åªéœ€æä¾›æ¶ˆæ¯ IDï¼Œå³æˆ‘ä»¬æœ‰ `to_fork.values["messages"].id` è¿™ä¸ªå€¼ã€‚


```python
fork_config = graph.update_state(
    to_fork.config,
    {"messages": [HumanMessage(content='Multiply 5 and 3', 
                               id=to_fork.values["messages"][0].id)]},
)
fork_config
```


```python
{'configurable': {'thread_id': '1',
  'checkpoint_ns': '',
  'checkpoint_id': '1ef6a442-3661-62f6-8001-d3c01b96f98b'}}
```

è¿™ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ã€åˆ†æ”¯å¼çš„æ£€æŸ¥ç‚¹ã€‚
ä½†æ˜¯ï¼Œå…ƒæ•°æ®â€”â€”æ¯”å¦‚æ¥ä¸‹æ¥è¯¥å»å“ªé‡Œâ€”â€”æ˜¯å¾—ä»¥ä¿ç•™ä¸‹æ¥çš„ï¼
æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„ä»£ç†ç¨‹åºçš„å½“å‰çŠ¶æ€å·²é€šè¿‡æˆ‘ä»¬çš„åˆ†æ”¯è¿›è¡Œäº†æ›´æ–°ã€‚


```python
all_states = [state for state in graph.get_state_history(thread) ]
all_states[0].values["messages"]
```


    [HumanMessage(content='Multiply 5 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d')]


```python
graph.get_state({'configurable': {'thread_id': '1'}})
```


```python
StateSnapshot(values={'messages': [HumanMessage(content='Multiply 5 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a442-3661-62f6-8001-d3c01b96f98b'}}, metadata={'source': 'update', 'step': 1, 'writes': {'__start__': {'messages': [HumanMessage(content='Multiply 5 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d')]}}, 'parents': {}}, created_at='2024-09-03T22:30:35.598707+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a440-a003-6c74-8000-8a2d82b0d126'}}, tasks=(PregelTask(id='f8990132-a8d3-5ddd-8d9e-1efbfc220da1', name='assistant', error=None, interrupts=(), state=None),))
```

ç°åœ¨ï¼Œåœ¨æˆ‘ä»¬è¿›è¡Œæµå¼ä¼ è¾“æ—¶ï¼Œè¯¥å›¾å°±ä¼šçŸ¥é“è¿™ä¸ªæ£€æŸ¥ç‚¹ä»æœªè¢«æ‰§è¡Œè¿‡ã€‚
æ‰€ä»¥ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯æŒç»­è¿›è¡Œçš„ï¼Œè€Œéåªæ˜¯ç®€å•åœ°é‡å¤æ’­æ”¾ã€‚


```python
for event in graph.stream(None, fork_config, stream_mode="values"):
    event['messages'][-1].pretty_print()
```

    ================================ Human Message =================================
    
    Multiply 5 and 3
    ================================== Ai Message ==================================
    Tool Calls:
      multiply (call_KP2CVNMMUKMJAQuFmamHB21r)
     Call ID: call_KP2CVNMMUKMJAQuFmamHB21r
      Args:
        a: 5
        b: 3
    ================================= Tool Message =================================
    Name: multiply
    
    15
    ================================== Ai Message ==================================
    
    The result of multiplying 5 and 3 is 15.


ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å½“å‰çŠ¶æ€æ˜¯æˆ‘ä»¬çš„ä»£ç†ç¨‹åºè¿è¡Œçš„ç»“æŸé˜¶æ®µã€‚


```python
graph.get_state({'configurable': {'thread_id': '1'}})
```


```python
StateSnapshot(values={'messages': [HumanMessage(content='Multiply 5 and 3', id='4ee8c440-0e4a-47d7-852f-06e2a6c4f84d'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KP2CVNMMUKMJAQuFmamHB21r', 'function': {'arguments': '{"a":5,"b":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 131, 'total_tokens': 148}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bc420009-d1f6-49b8-bea7-dfc9fca7eb79-0', tool_calls=[{'name': 'multiply', 'args': {'a': 5, 'b': 3}, 'id': 'call_KP2CVNMMUKMJAQuFmamHB21r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 131, 'output_tokens': 17, 'total_tokens': 148}), ToolMessage(content='15', name='multiply', id='9232e653-816d-471a-9002-9a1ecd453364', tool_call_id='call_KP2CVNMMUKMJAQuFmamHB21r'), AIMessage(content='The result of multiplying 5 and 3 is 15.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 156, 'total_tokens': 170}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-86c21888-d832-47c5-9e76-0aa2676116dc-0', usage_metadata={'input_tokens': 156, 'output_tokens': 14, 'total_tokens': 170})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a442-a2e2-6e98-8004-4a0b75537950'}}, metadata={'source': 'loop', 'writes': {'assistant': {'messages': [AIMessage(content='The result of multiplying 5 and 3 is 15.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 156, 'total_tokens': 170}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-86c21888-d832-47c5-9e76-0aa2676116dc-0', usage_metadata={'input_tokens': 156, 'output_tokens': 14, 'total_tokens': 170})]}}, 'step': 4, 'parents': {}}, created_at='2024-09-03T22:30:46.976463+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef6a442-9db0-6056-8003-7304cab7bed8'}}, tasks=())
```

